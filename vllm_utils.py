import requests
import re

# âœ… vLLM API ì„œë²„ ì •ë³´
VLLM_API_URL = "http://localhost:8000/v1/completions"
MODEL_ID = "/home/filadmin/ai-project/vllm/production-models/gemma-3-27b-it"

# âœ… 1ï¸âƒ£ vLLM API í˜¸ì¶œ í•¨ìˆ˜
def call_vllm(prompt, max_tokens=256, stop=None):
    try:
        response = requests.post(
            VLLM_API_URL,
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_ID,
                "prompt": prompt.strip(),
                "max_tokens": max_tokens,
                "temperature": 0.4,
                **({"stop": stop} if stop else {})
            },
            timeout=30
        )

        response.raise_for_status()
        result = response.json()
        print("ğŸ” vLLM ì‘ë‹µ ì „ì²´:", result)

        choices = result.get("choices", [])
        if choices and "text" in choices[0]:
            return choices[0].get("text", "").strip()

        return "[âš ï¸ LLM ì‘ë‹µì— í…ìŠ¤íŠ¸ ì—†ìŒ]"

    except requests.RequestException as e:
        print(f"[âŒ vLLM í˜¸ì¶œ ì‹¤íŒ¨]: {e}")
        return "[âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨]"


# âœ… 2ï¸âƒ£ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± í•¨ìˆ˜
def call_vllm_generate_search_condition(user_question):
    prompt = f"""
ë‹¤ìŒì€ ë¬¸ì„œ ê²€ìƒ‰ìš© í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì´ì•¼.
â—ï¸ì ˆëŒ€ ì„¤ëª…í•˜ì§€ ë§ê³ , ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ëª©ë¡ë§Œ ìƒì„±í•´.

ê·œì¹™:
- ì§ˆë¬¸ì— ëª…ì‹œëœ ì—°ë„ê°€ ìˆì„ ë•Œë§Œ í¬í•¨í•´. ì—†ìœ¼ë©´ ì—°ë„ëŠ” ì ˆëŒ€ ë„£ì§€ ë§ˆ.
- ì—°ë„ëŠ” í•­ìƒ 4ìë¦¬ ìˆ«ì (ì˜ˆ: '23ë…„ë„' â†’ '2023')
- ì›”,ì¼ì´ ë“¤ì–´ê°€ë©´ ì•ì— ìˆ«ìë§Œ ì¶”ì¶œí•´ì¤˜
- HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì, ê°œí–‰ë¬¸ì(\\n), ë”°ì˜´í‘œ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ
- ì¶œë ¥ì€ ì˜ˆ: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3 í˜•ì‹ì´ì–´ì•¼ í•¨

ì§ˆë¬¸: {user_question}

í‚¤ì›Œë“œ:"""
    return call_vllm(prompt, max_tokens=32, stop=["\n"])


# âœ… 3ï¸âƒ£ í‚¤ì›Œë“œ í›„ì²˜ë¦¬ í•¨ìˆ˜
def clean_llm_keywords(raw_text: str) -> list:
    first_line = raw_text.strip().split("\n")[0]  # ì²« ì¤„ë§Œ ì‚¬ìš©
    cleaned = re.sub(r"(?i)ì§ˆë¬¸\s*:.*", "", first_line)  # "ì§ˆë¬¸:" ì œê±°
    cleaned = re.sub(r"<[^>]+>", "", cleaned)
    cleaned = re.sub(r"[\\\n\r\t]", " ", cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return [kw.strip() for kw in cleaned.split(",") if kw.strip()]


# âœ… 4ï¸âƒ£ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ í•¨ìˆ˜
def call_vllm_summarize_article(article_text, user_question=None):
    cleaned_text = clean_article_text(article_text)
    prompt = f"""ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ë³¸ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ì¡°ê±´:
- ë¬¸ì¥ ë°˜ë³µ ê¸ˆì§€
- "ìš”ì•½" ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš© ê¸ˆì§€
- ë¬¸ë²•ê³¼ ì–´íœ˜ë¥¼ ìœ ì—°í•˜ê²Œ

[ë³¸ë¬¸]
{cleaned_text}
"""
    raw_summary = call_vllm(prompt, max_tokens=512)  # âŒ stop ì œê±°
    return clean_sentences_preserve_meaning(raw_summary)




# âœ… 5ï¸âƒ£ ë¬¸ì¥ ì •ì œ í•¨ìˆ˜
def clean_sentences_preserve_meaning(text: str) -> str:
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[\r\n\t]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# âœ… 6ï¸âƒ£ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
def clean_article_text(text: str) -> str:
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = text.replace('â€œ', '"').replace('â€', '"')
    text = text.replace("â€˜", "'").replace("â€™", "'")
    text = re.sub(r"\([^)]{0,30}\)", "", text)
    text = re.sub(r"[â€¢â˜…â˜†â–¶â–²â–¼â†’â€»]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text
